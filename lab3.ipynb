{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZOEGXLohW2Lo3oqKIdO0q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bTKnlWO8iPuc"},"outputs":[],"source":["# --- Installation des dépendances ---\n","!pip install beautifulsoup4 requests pandas nltk pyarabic\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.isri import ISRIStemmer\n","import pyarabic.araby as araby\n","nltk.download('stopwords')\n","\n","# --- Fonction de scraping simple ---\n","def scrape_articles(urls, max_per_site=50):\n","    texts = []\n","    for url in urls:\n","        try:\n","            headers = {'User-Agent': 'Mozilla/5.0'}\n","            response = requests.get(url, headers=headers)\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            paragraphs = soup.find_all('p')\n","            for p in paragraphs[:max_per_site]:\n","                text = p.get_text(strip=True)\n","                if len(text) > 50:  # garder seulement les paragraphes significatifs\n","                    texts.append(text)\n","        except:\n","            continue\n","    return texts\n","\n","# Sites arabes politiques\n","urls = [\n","    \"https://www.aljazeera.net/politics/\",\n","    \"https://arabic.cnn.com/politics\",\n","    \"https://www.hespress.com/politique/\"\n","]\n","\n","raw_texts = scrape_articles(urls, max_per_site=100)\n","print(f\"Nombre de paragraphes collectés : {len(raw_texts)}\")\n","\n","# --- Attribution manuelle des scores (exemple simplifié) ---\n","# Ici on simule l'attribution manuelle. En pratique, tu le fais à la main ou semi-automatiquement.\n","import random\n","data = []\n","keywords_high = [\"انتخابات\", \"حكومة\", \"برلمان\", \"أخنوش\", \"بايدن\", \"فلسطين\", \"إسرائيل\"]\n","keywords_low = [\"كرة\", \"مباراة\", \"فن\", \"موسيقى\"]\n","\n","for text in raw_texts[:200]:  # limiter pour l'exemple\n","    score = 5.0\n","    if any(k in text for k in keywords_high):\n","        score += random.uniform(2, 5)\n","    if any(k in text for k in keywords_low):\n","        score -= random.uniform(2, 4)\n","    score = max(0, min(10, score))\n","    data.append({\"text\": text, \"score\": round(score, 1)})\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"arabic_politics_dataset.csv\", index=False)\n","print(df.head(10))\n","\n","# --- Preprocessing pipeline ---\n","stop_words = set(stopwords.words('arabic'))\n","stemmer = ISRIStemmer()\n","\n","def preprocess_arabic(text):\n","    # Nettoyage\n","    text = araby.strip_tashkeel(text)\n","    text = araby.normalize_hamza(text, method=\"tasheel\")\n","    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  # garder seulement arabe + espaces\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    # Tokenization\n","    tokens = araby.tokenize(text)\n","\n","    # Stop words + stemming\n","    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words and len(token) > 2]\n","\n","    return \" \".join(tokens)\n","\n","df['clean_text'] = df['text'].apply(preprocess_arabic)\n","df.to_csv(\"arabic_politics_dataset_clean.csv\", index=False)\n","print(\"Dataset nettoyé sauvegardé !\")\n","print(df[['text', 'clean_text', 'score']].head())"]}]}